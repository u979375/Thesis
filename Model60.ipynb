{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWisdNsBUJKe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2EHlaxnVq38"
   },
   "outputs": [],
   "source": [
    "#reading images' filenames\n",
    "seed_SH = 22\n",
    "path = os.path.abspath(os.getcwd())\n",
    "imgset_0_path = path+'/White/*.jpg'\n",
    "imgset_1_path = path+'/Black/*.jpg'\n",
    "imgset_2_path = path+'/Asian/*.jpg'\n",
    "imgset_3_path = path+'/Indian/*.jpg'\n",
    "imgset_4_path = path+'/Others/*.jpg'\n",
    "\n",
    "filename_dataset0 = tf.data.Dataset.list_files(imgset_0_path, seed=seed_SH)\n",
    "filename_dataset1 = tf.data.Dataset.list_files(imgset_1_path, seed=seed_SH)\n",
    "filename_dataset2 = tf.data.Dataset.list_files(imgset_2_path, seed=seed_SH)\n",
    "filename_dataset3 = tf.data.Dataset.list_files(imgset_3_path, seed=seed_SH)\n",
    "filename_dataset4 = tf.data.Dataset.list_files(imgset_4_path, seed=seed_SH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SF72Jz7NV3dn"
   },
   "outputs": [],
   "source": [
    "#creating equal distribution dataset\n",
    "index = min(len(list(filename_dataset0)), len(list(filename_dataset1)), len(list(filename_dataset2)), len(list(filename_dataset3)), len(list(filename_dataset4)))\n",
    "filename_dataset0 = filename_dataset0.take(index)  \n",
    "filename_dataset1 = filename_dataset1.take(index)  \n",
    "filename_dataset2 = filename_dataset2.take(index)  \n",
    "filename_dataset3 = filename_dataset3.take(index)  \n",
    "filename_dataset4 = filename_dataset4.take(index) \n",
    "\n",
    "filename_dataset = filename_dataset0.concatenate(filename_dataset1)\n",
    "filename_dataset = filename_dataset.concatenate(filename_dataset2)\n",
    "filename_dataset = filename_dataset.concatenate(filename_dataset3)\n",
    "filename_dataset = filename_dataset.concatenate(filename_dataset4)\n",
    "filename_dataset = filename_dataset.shuffle(buffer_size=10000, seed=42, reshuffle_each_iteration=False) \n",
    "\n",
    "# number of files to work with as data\n",
    "nr_files = int(index*5*0.6)\n",
    "\n",
    "#get 60% of original dataset as dataset to work with\n",
    "filename_dataset = filename_dataset.take(nr_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCR6eXj_Wrco"
   },
   "outputs": [],
   "source": [
    "#read filename and return resized normalized image with label\n",
    "def load_image(image_path):\n",
    "    label = tf.strings.split(image_path, sep='_')[2]\n",
    "    label = tf.strings.to_number(label,out_type=tf.dtypes.int32)\n",
    "    image = tf.io.read_file(image_path)  \n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.dtypes.cast(image,tf.float32)\n",
    "    image = image/tf.math.reduce_max(image)\n",
    "    return (image,label)\n",
    "\n",
    "img_label = filename_dataset.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFw2SZ2WXUFh"
   },
   "outputs": [],
   "source": [
    "#check distribution of this dataset\n",
    "white, black, indian, asian, others = 0,0,0,0,0\n",
    "for i,j in img_label:\n",
    "    if j == tf.Variable(0):\n",
    "        white = white+1\n",
    "    if j == tf.Variable(1):\n",
    "        black = black+1\n",
    "    if j == tf.Variable(2):\n",
    "        asian = asian+1\n",
    "    if j == tf.Variable(3):\n",
    "        indian = indian+1\n",
    "    if j == tf.Variable(4):\n",
    "        others = others+1\n",
    "np.std([white, black, asian, indian, others])\n",
    "# print(white, black, asian,indian, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRdU8jtRY6SQ"
   },
   "outputs": [],
   "source": [
    "#split data into train, validation and test\n",
    "split  = tf.math.round(nr_files*0.8) # is 80% of data\n",
    "split  = tf.dtypes.cast(split,tf.int64)\n",
    "train0 = img_label.take(split)\n",
    "split  = tf.dtypes.cast(split,tf.float32)\n",
    "val_split = tf.math.round(split * 0.75)\n",
    "val_split = tf.dtypes.cast(val_split,tf.int64)\n",
    "train = train0.take(val_split)\n",
    "val   = train0.skip(val_split)\n",
    "test  = img_label.skip(tf.dtypes.cast(split, tf.int64))\n",
    "\n",
    "train = train.batch(64).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val = val.batch(64).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test = test.batch(64).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OaDo3AtVY4hr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#building model\n",
    "base_model = tf.keras.applications.VGG16(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(320, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(5, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZzyY1DfZu7m"
   },
   "outputs": [],
   "source": [
    "# compile, fit and evaluate model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']) \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,restore_best_weights=True)\n",
    "history = model.fit(train, validation_data=(val), epochs=200, verbose=1)\n",
    "scores = model.evaluate(val)\n",
    "print(\"%suracy validation set: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy score of test set\n",
    "labels = []\n",
    "for i, j in test:\n",
    "    for a in j:\n",
    "        labels.append(a.numpy()) \n",
    "labels = np.array(labels) \n",
    "\n",
    "results = model.predict(test)\n",
    "pred_labels = tf.argmax(results, axis=1, output_type=tf.int32)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "m = tf.keras.metrics.Accuracy() \n",
    "_ = m.update_state(labels, pred_labels)\n",
    "acc_test = m.result().numpy() \n",
    "print('accuracy test set: ', str(round(acc_test*100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss function\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(val_loss) + 1)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of categories in test set\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, counts))\n",
    "\n",
    "#distribution of categories predicted from the test set\n",
    "unique, counts = np.unique(pred_labels, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEE7hpwtbnd3"
   },
   "outputs": [],
   "source": [
    "# returns roc_auc score for each race category\n",
    "def auc_score(pred_labels, labels, cat):\n",
    "    if cat in pred_labels:\n",
    "        pred = np.where(pred_labels==cat, 1, 0)\n",
    "        true = np.where(labels==cat, 1, 0)\n",
    "    else:\n",
    "        return print('this category has not been predicted')\n",
    "    return roc_auc_score(true, pred)\n",
    "\n",
    "#get roc_auc scores for each category in list\n",
    "auc_scores = []\n",
    "for i in range(5):\n",
    "    auc_scores.append(auc_score(pred_labels, labels, i))\n",
    "print('List of ROC AUC scores for White, Black, Asian, Indian and Others category', auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns for each falsely predicted category the number of times it was falsely predicted for each actual category\n",
    "comparison = list(zip(pred_labels, labels))\n",
    "def get_FN(comparison, cat): \n",
    "    freq_FN = {}\n",
    "    for i,j in comparison:\n",
    "        if j == cat and i != cat:\n",
    "            if i in freq_FN:\n",
    "                freq_FN[i] += 1\n",
    "            else:\n",
    "                freq_FN[i] = 1\n",
    "    return freq_FN\n",
    "\n",
    "categories= []\n",
    "for i in range(5):\n",
    "    FN = get_FN(comparison, i)\n",
    "    categories.append(('category '+str(i), 'FN',FN))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPn9aPLX4t4LmTFYtvhGFc3",
   "collapsed_sections": [],
   "name": "Model_20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
